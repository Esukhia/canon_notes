Before starting to categorise, I reduce the text of the note to the maximum, meaning that I take off all the syllables that are found in all the editions of the current text and try to fit in one of the following categories whatever is left of the text of the note.
    A. "automatic_categorisation": cat of notes automatically categorised
        1 "min_mod": group of modifications that are estimated to be of minimal impact of the understanding of the passage.
			All sub-categories of 1 were put together after grouping the notes that I had manually categorised as min_mod (minor modifications) within all the notes of སྤྱོད་འཇུག text.
            1.1 "min_mod_groups": Group of syllables that may or may not contain a particle.
            1.2 "particle_groups": Group of pairs or triplets of particles
        2 "particle_issues": This category takes all the notes where the difference between the editions is only a particle. Only the notes that don’t fit in 1.2 come here.
            "added_particle": There is an extra particle in one or more of the editions and nothing in at least one of the other editions
            "agreement_issue": All the particles in all the editions are from the same case but are different.
            "po-bo-pa-ba": The difference is a combination of po/bo/pa/ba
            "different_particles": The cases in the text is different cases of particles
            "other": All other cases of notes with only cases that don’t enter in either of the above categories.
        3 "spelling_mistake": All the notes containing a # showing there is a mistake detected by our Python segmentation script
            "missing_vowel": Adding a vowel after the main stack makes it so that there is no more spelling mistake detected by the segmentation script (the whole note checked, not only the differing syllables) 
            "nga_da": Changing a nga into a da makes it so that there is no more spelling mistake detected by the segmentation script (the whole note checked, not only the differing syllables) 
            "non_word": All the notes that don’t enter in either missing_vowel or nga_da or sskrt.
                "ill_formed": The syllable detected as a mistake is not a valid Tibetan syllable (I try to split the syllable in two with Segment.get_syl_components(). If I have no output, the syllable is not valid)
                "well_formed": The syllable is a valid Tibetan syllable (notes in this section are good candidates for words that are not yet added in the segmentation script’s lexicon)
        4 "sskrt": The syllable detected as a mistake is caught by the three regexes of Hackett to detect sskrt text
        5 "verb_difference": The differing syllables are only verbs. I am using the verb entries of the new Monlam dictionary that I have parsed. It seems འགོག་ has not been extracted from the dictionary. I need to check what went wrong.
            "diff_tense": The verbs in the different editions all pertain to the same verb, but are conjugated at different tenses
            "diff_verb": The verbs are different
            "not_sure": All the cases where things are not clear but the differing syllables are indeed verbs
    6 "dunno": All the notes that enter in any of all the other categories, including empty_notes. In an effort to still group similar notes, I compared the number of differing syllables for all the editions
        "long_diff": The difference of syllables is higher than 2 (at least one edition is longer than the others by two syllables or more)
        "no_diff": The editions have the same amount of differing syllables
        "short_diff": At least one edition is longer than the others by either 1 or 2 syllables
    7 "empty_notes": Notes where the text of the notes (not the differing syllables) is an empty string. (I still didn’t manage to see how come this is possible)
    8 "manual_categorisation": The notes in this section will be categorised by a human reviewer and added here in order to calculate the final decision to either keep or discard the notes.
        "long_difference": The long notes that are in dunno/long_diff will be proposed to the human reviewer to categorise as either a differing formulation or a major modification
            "differing_formulation": 
            "major_modification": 
        "manual_evaluation": For each note that the reviewer checks, he will be asked to note either of 'y' (derge is obviously correct), 'a' (the difference of meaning in the different editions is great), 'b' (the meaning difference is medium) or 'c' (the meaning difference is small)
            "derge_correct": y
            "meaning_difference": 
                "great_diff": a
                "medium_diff": b
                "small_diff": c
    9 "ngram_freq": a dict with a pair for all the notes. I generate all combination of syllables starting from the text of the note and incrementing progressively a syllable left, then another on the right etc. I do the same starting from the right. I then check for every of the variants if they are found in the list of all the ngrams from 1-gram to 12-grams of the Kangyur. (the ngrams of the tengyur and at least of the nalanda corpus should also be generated). I thus create a list of all the ngram found and each of them is a tuple consisting of the ngram text and its frequency in the kangyur.
    10 "non_standard_notes": The human reviewer will have the opportunity to write a note that will differ from the standard one (automatically generated) for cases where it is necessary.
    11 "profile": For every note, I check how many editions agree and how many disagree. This becomes something like 'derge=cone pecing=narthang'. Here, derge and cone agree, pecing and narthang agree, but the two groups have different spellings. 'derge cone pecing narthang' means that every edition writes something different from all others. With each profile I provide the percentage of this profile in the current text.
